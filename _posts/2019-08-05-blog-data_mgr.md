---
layout: post
title: 数据管理-冷热分离
date: 2019-08-05 11:47:57
img: workflow.jpg
category: blog
tags: [blog]
---
数据太大，影响维护时间，影响数据回归时间，影响数据备份还原等。

## 大数据的产生

1. 运营时间久
2. 新增登录较高的游戏
3. 游戏处于同一个集群环境

<br/>

## 冷热数据分离

设定3个月未登录的用户为冷数据，这部分数据将不会再内存中加载。
当上次登录时间大于3个月的用户登录，可以从冷数据库获得数据。

## 以下为数据的时间段占比
![avatar](/assets/img/db_1.jpg)
![avatar](/assets/img/db_2.jpg)

## 设计方式

1. 本地存放
本地数据库，对应数据表，支持冷表和热表。
冷表提供数据激活接口，在业务层判定数据冷热状态，并决定是否调用激活数据。

2. 远端存放
远端启动一个数据，在维护启动数据加载的时候，判定数据状态，向远端备份库推送数据。
在需要使用冷数据的时候访问远端数据库获得数据。

## 问题

1. 始终有一张用于标记数据状态的数据表，不会冷热分离，需要考虑它的瓶颈。
2. 冷数据在使用的时候，需要注意业务层是否需要迭代的问题 （原则上不满足，可以调整冷热时间）。
3. 数据管理中心，需要查看数据问题（比如中控和集群管理页面，也可能是GMC或日志）

## 微信后台基于时间序的海量数据冷热分级架构设计实践

微信内大部分业务生成的数据是有共性可言的：数据键值带有时间戳信息，并且单用户数据随着时间在不断的生成。我们将这类数据称为基于时间序的数据。

1. 在微信的实际应用场景中，这类数据的主要特点包括：数据量大、访问量大、重要程度高等。这些特点在现网的实际运营过程中，给我们带来了非常大的挑战，主要包括：

2. 数据量大，需求的存储容量高――基于时间序的数据通常不会删除，而是随着时间不断积累，数据量达到 PB 级别，相应需要的存储空间也与日俱增；

3. 访问量大，节日效应明显――基于时间序的数据往往是热点业务生成的数据，它们的访问量居高不下，基本维持在每分钟数十亿次的级别。尤其是在节日期间，瞬发访问量更可达平日的三至五倍；

4. 重要性高，用户感知明显，数据一旦丢失，导致用户不能正常使用产品，并因此而转化成的投诉率高。

通过堆机器来横向扩展存储自然可以应对如上的各种挑战，然而在成本预算紧张的前提下，机器数目是有限的。在这种情况下，基于时间序的海量数据的冷热分级架构便应运而生。

对热点数据，数据量少，但承担的访问流量大，我们当然是希望它们能常驻内存，因此系统提供了有强一致保证的内存层，在应对突发流量时，也可在不涉及历史数据迁移的前提下，单独、动态的快速扩展内存层。

对历史数据，数据存量大，但承担的访问量非常有限，我们当然是不希望用昂贵的固态硬盘来存储它们，因此，系统提供了廉价的机械盘层，并且有一套透明的冷数据剥离和批量下沉的流程，将存储层中历史数据源源不断的抽离到机械盘层。

## 数据层级

系统由三个层次组成，内存层、存储层（热数据存储层，冷数据存储层）、

1. 数据强一致性保证
 
业务要求系统必须保证在数据的多份副本之间保持强一致性。――这是一个历久弥新的挑战。我们将分内存层、存储层、机械硬盘层分别来考虑数据的强一致性维持。

2. 强一致缓存
 
正如前文描述，内存层作为一种强一致性分布式缓存，它完全是向存储层对齐的，自身无法判别数据有效性，本身多副本之间也没有交互的必要。它对前端而言是只读的，所有的写请求并不通过它，它只能算是存储层中数据的一个视图。所以它对前端数据有效性的承诺完全是依赖于存储层的正确性的。